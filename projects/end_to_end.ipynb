{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End to End Pipeline with YOLO + OCRs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils function for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchmetrics.text import CharErrorRate\n",
    "import jiwer\n",
    "THAI_COLORS = [\n",
    "    \"แดง\",    # Red\n",
    "    \"น้ำเงิน\", # Blue\n",
    "    \"เขียว\",  # Green\n",
    "    \"เหลือง\", # Yellow\n",
    "    \"ส้ม\",    # Orange\n",
    "    \"ชมพู\",   # Pink\n",
    "    \"ม่วง\",   # Purple\n",
    "    \"น้ำตาล\", # Brown\n",
    "    \"ขาว\",    # White\n",
    "    \"ดำ\",     # Black\n",
    "    \"เทา\",    # Gray\n",
    "    \"ฟ้า\",    # Light Blue\n",
    "    \"ทอง\",    # Gold\n",
    "    \"เงิน\",   # Silver\n",
    "]\n",
    "THAI_PROVINCES = [\n",
    "    \"กรุงเทพมหานคร\",\n",
    "    \"สมุทรปราการ\",\n",
    "    \"นนทบุรี\",\n",
    "    \"ปทุมธานี\",\n",
    "    \"พระนครศรีอยุธยา\",\n",
    "    \"อ่างทอง\",\n",
    "    \"ลพบุรี\",\n",
    "    \"สิงห์บุรี\",\n",
    "    \"ชัยนาท\",\n",
    "    \"สระบุรี\",\n",
    "    \"ชลบุรี\",\n",
    "    \"ระยอง\",\n",
    "    \"จันทบุรี\",\n",
    "    \"ตราด\",\n",
    "    \"ฉะเชิงเทรา\",\n",
    "    \"ปราจีนบุรี\",\n",
    "    \"นครนายก\",\n",
    "    \"สระแก้ว\",\n",
    "    \"นครราชสีมา\",\n",
    "    \"บุรีรัมย์\",\n",
    "    \"สุรินทร์\",\n",
    "    \"ศรีสะเกษ\",\n",
    "    \"อุบลราชธานี\",\n",
    "    \"ยโสธร\",\n",
    "    \"ชัยภูมิ\",\n",
    "    \"อำนาจเจริญ\",\n",
    "    \"หนองบัวลำภู\",\n",
    "    \"ขอนแก่น\",\n",
    "    \"อุดรธานี\",\n",
    "    \"เลย\",\n",
    "    \"หนองคาย\",\n",
    "    \"มหาสารคาม\",\n",
    "    \"ร้อยเอ็ด\",\n",
    "    \"กาฬสินธุ์\",\n",
    "    \"สกลนคร\",\n",
    "    \"นครพนม\",\n",
    "    \"มุกดาหาร\",\n",
    "    \"เชียงใหม่\",\n",
    "    \"ลำพูน\",\n",
    "    \"ลำปาง\",\n",
    "    \"อุตรดิตถ์\",\n",
    "    \"แพร่\",\n",
    "    \"น่าน\",\n",
    "    \"พะเยา\",\n",
    "    \"เชียงราย\",\n",
    "    \"แม่ฮ่องสอน\",\n",
    "    \"นครสวรรค์\",\n",
    "    \"อุทัยธานี\",\n",
    "    \"กำแพงเพชร\",\n",
    "    \"ตาก\",\n",
    "    \"สุโขทัย\",\n",
    "    \"พิษณุโลก\",\n",
    "    \"พิจิตร\",\n",
    "    \"เพชรบูรณ์\",\n",
    "    \"ราชบุรี\",\n",
    "    \"กาญจนบุรี\",\n",
    "    \"สุพรรณบุรี\",\n",
    "    \"นครปฐม\",\n",
    "    \"สมุทรสาคร\",\n",
    "    \"สมุทรสงคราม\",\n",
    "    \"เพชรบุรี\",\n",
    "    \"ประจวบคีรีขันธ์\",\n",
    "    \"นครศรีธรรมราช\",\n",
    "    \"กระบี่\",\n",
    "    \"พังงา\",\n",
    "    \"ภูเก็ต\",\n",
    "    \"สุราษฎร์ธานี\",\n",
    "    \"ระนอง\",\n",
    "    \"ชุมพร\",\n",
    "    \"สงขลา\",\n",
    "    \"สตูล\",\n",
    "    \"ตรัง\",\n",
    "    \"พัทลุง\",\n",
    "    \"ปัตตานี\",\n",
    "    \"ยะลา\",\n",
    "    \"นราธิวาส\",\n",
    "    \"บึงกาฬ\"\n",
    "]\n",
    "\n",
    "\n",
    "def calculate_cer(predictions: list[str], labels: list[str]) -> list[float]:\n",
    "    \"\"\"Return the Character Error Rate (CER) between the predicted and target strings.\"\"\"\n",
    "    cer = CharErrorRate()\n",
    "    cer_val = cer(predictions, labels)\n",
    "    return cer_val.tolist()\n",
    "\n",
    "def evaluate(label_df: pd.DataFrame, prediction_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Merge some of the columnof label_df to match with prediction_df\n",
    "    ## Merge 'plate1' and 'plate2' to 'registration_no'\n",
    "    label_df[\"registration_no\"] = label_df[\"plate1\"] + \" \" +label_df[\"plate2\"]\n",
    "    label_df = label_df.drop(columns=[\"plate1\", \"plate2\"])\n",
    "    ## Merge 'axles_wheels_no', 'wheels', 'tires' to 'axles_wheels_no'\n",
    "    ## e.g. 'x x x' -> 'x เพลา x ล้อ ยาง x เส้น'\n",
    "    label_df[\"axles_wheels_no\"] = label_df[\"axles_wheels_no\"].apply(lambda x: x + \" เพลา\")\n",
    "    label_df[\"wheels\"] = label_df[\"wheels\"].apply(lambda x: x + \" ล้อ\")\n",
    "    label_df[\"tires\"] = label_df[\"tires\"].apply(lambda x: \"ยาง \" + x + \" เส้น\")\n",
    "    label_df[\"axles_wheels_no\"] = label_df[\"axles_wheels_no\"] + \" \" + label_df[\"wheels\"] + \" \" + label_df[\"tires\"]\n",
    "    label_df = label_df.drop(columns=[\"wheels\", \"tires\"])\n",
    "    # Rename some columns of label_df to match with prediction_df\n",
    "    rename_dict = {\n",
    "        \"province\": \"car_province\",\n",
    "        \"type_car\": \"vehicle_use\",\n",
    "        \"kind\": \"body_style\",\n",
    "        \"num_body\": \"chassis_number\",\n",
    "        \"brand\": \"manufacturer\",\n",
    "        \"num_engine\": \"engine_number\",\n",
    "    }\n",
    "    label_df = label_df.rename(columns=rename_dict)\n",
    "    # Replace any NaN with ''\n",
    "    label_df = label_df.fillna(\"\")\n",
    "    prediction_df = prediction_df.fillna(\"\")\n",
    "    # Replace 'ไม่พบข้อมูล' with ''\n",
    "    prediction_df[\"year\"] = prediction_df[\"year\"].map(lambda x: x.replace(\"ไม่พบข้อมูล\", \"\"))\n",
    "    label_df[\"year\"] = label_df[\"year\"].map(lambda x: x.replace(\"ไม่พบข้อมูล\", \"\"))\n",
    "    # Rearrange the columns\n",
    "    columns_of_interest = [\n",
    "        'date_of_registration', 'registration_no', 'car_province', 'vehicle_use', 'type', 'body_style',\n",
    "        'manufacturer', 'model', 'year', 'color', 'chassis_number', 'chassis_location', 'engine_manufacturer',\n",
    "        'engine_number', 'engine_location', 'fuel_type', 'fuel_tank_number', 'cylinders', 'cubic_capacity',\n",
    "        'horse_power', 'axles_wheels_no', 'unladen_weight', 'load_capacity', 'gross_weight', 'seats'\n",
    "    ]\n",
    "    label_df = label_df[columns_of_interest]\n",
    "    prediction_df = prediction_df[columns_of_interest]\n",
    "    # Create index column on both dataframes\n",
    "    label_df[\"index\"] = range(len(label_df))\n",
    "    prediction_df[\"index\"] = range(len(prediction_df))\n",
    "    # Evaluate\n",
    "    merged_df = pd.merge(label_df, prediction_df, on=\"index\", suffixes=('_annotation', '_prediction'))\n",
    "    merged_df.to_csv(\"merged_df.csv\")\n",
    "    eval_list = []\n",
    "    for col in columns_of_interest:\n",
    "        if f\"{col}_annotation\" in merged_df.columns and f\"{col}_prediction\" in merged_df.columns:\n",
    "            avg_cer = np.mean(calculate_cer(merged_df[f\"{col}_prediction\"], merged_df[f\"{col}_annotation\"]))\n",
    "            avg_accuracy = (merged_df[f\"{col}_prediction\"] == merged_df[f\"{col}_annotation\"]).mean() * 100\n",
    "            eval_list.append({\n",
    "                \"column_name\": col,\n",
    "                \"cer\": avg_cer,\n",
    "                \"accuracy\": avg_accuracy\n",
    "            })\n",
    "    eval_df = pd.DataFrame(eval_list)\n",
    "    return eval_df\n",
    "\n",
    "def map_with_cer(input_text: str, templates: list[str], cer_threshold: float = 0.5) -> str:\n",
    "    \"\"\"Try to map the input_text with the templates using CER. If the CER is below the threshold, return the template, else return the input_text.\"\"\"\n",
    "    if input_text == \"\":\n",
    "        return \"\"\n",
    "    cer_list = [jiwer.cer(template, input_text) for template in templates]\n",
    "    # Remove the templatse if the CER is greater than the threshold\n",
    "    templates = [template for i, template in enumerate(templates) if cer_list[i] < cer_threshold]\n",
    "    cer_list = [cer for cer in cer_list if cer < cer_threshold]\n",
    "    if len(cer_list) == 0:\n",
    "        return input_text\n",
    "    return templates[np.argmin(cer_list)]\n",
    "    \n",
    "def postprocess_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Post-process the dataframe\n",
    "    # Apply `map_with_cer` fuel_type\n",
    "    df[\"color\"] = df[\"color\"].apply(lambda x: map_with_cer(x, THAI_COLORS, cer_threshold=0.4))\n",
    "    df[\"fuel_type\"] = df[\"fuel_type\"].apply(lambda x: map_with_cer(x, [\"ดีเซล\", \"เบนซิน\"], cer_threshold=0.8))\n",
    "    df[\"body_style\"] = df[\"body_style\"].apply(lambda x: map_with_cer(x, ['กระบะบรรทุก', 'กระบะบรรทุก (ไม่มีหลังคา)', 'รถจักรยานยนต์', 'นั่งสองตอนท้ายบรรทุก', 'กระบะบรรทุก (ติดตั้งโครงเหล็ก)', 'กระบะบรรทุก  (ไม่มีหลังคา)', 'เก๋งสองตอน', 'รถแทรกเตอร์ที่ใช้ในการเกษตร', 'นั่งสามตอน'], cer_threshold=0.8))\n",
    "    df[\"vehicle_use\"] = df[\"vehicle_use\"].apply(lambda x: map_with_cer(x,['รถยนต์บรรทุกส่วนบุคคล', 'รถจักรยานยนต์', 'รถยนต์นั่งส่วนบุคคลไม่เกิน 7 คน', 'รถแทรกเตอร์', 'รถจักรยานยนต์สาธารณะ'], cer_threshold=0.5))\n",
    "    df[\"car_province\"] = df[\"car_province\"].apply(lambda x: map_with_cer(x, THAI_PROVINCES, cer_threshold=0.7))\n",
    "    df[\"engine_location\"] = df[\"engine_location\"].apply(lambda x: map_with_cer(x,['ขวาเครื่อง', 'ซ้ายเครื่อง', 'ใต้ที่นั่ง', 'ซ้ายเรื่อง', 'ล่างเครื่อง'], cer_threshold=0.6))\n",
    "    df[\"chassis_location\"] = df[\"chassis_location\"].apply(lambda x: map_with_cer(x, ['กลางขวา', 'หลังขวา', 'หน้าขวา', 'ใต้ที่นั่ง', 'หน้าซ้าย', 'หลัง', 'คอบังคับเลี้ยว', 'กระจังหน้าตอนใน'], cer_threshold=0.8))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine dataset directory\n",
    "\n",
    "Location where the dataset is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "dataset_dir = Path(\"../datasets/Data/Srisawad_Dataset_100\")\n",
    "image_paths = list(dataset_dir.glob(\"*.jpg\"))\n",
    "\n",
    "# Sort just to make it beautiful >.<\n",
    "image_paths = list(sorted(image_paths))\n",
    "image_paths[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get bboxes from YOLO first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load model\n",
    "model = YOLO(\"../assets/best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_results = model.predict(image_paths, imgsz=640, conf=0.25, half=True, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crop textboxes function from YOLO results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def get_textboxes(result) -> list[Image.Image]:\n",
    "    # Get image\n",
    "    image = Image.fromarray(result.orig_img)\n",
    "    # Get bounding boxes\n",
    "    bboxes = result.boxes.data[:, :4].round().cpu().int().numpy().tolist()\n",
    "\n",
    "    # Get all textbox images\n",
    "    textbox_images = []\n",
    "    for bbox in bboxes:\n",
    "        # Crop image\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        textbox_image = image.crop((x1, y1, x2, y2))\n",
    "        textbox_images.append(textbox_image)\n",
    "    return textbox_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Surya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surya.model.recognition.model import load_model as load_recognizer\n",
    "from surya.model.recognition.processor import load_processor as load_recognizer_processor\n",
    "\n",
    "recognizer = load_recognizer()\n",
    "recognizer_processor = load_recognizer_processor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from surya.recognition import batch_recognition\n",
    "\n",
    "surya_document_data = []\n",
    "# Detect bboxes for each document\n",
    "for result in tqdm(yolo_results):\n",
    "    # Get textboxes\n",
    "    textbox_images = get_textboxes(result)\n",
    "\n",
    "    # Get bounding boxes, class labels, and scores\n",
    "    class_names = result.names\n",
    "    class_predictions = result.boxes.cls.cpu().int().numpy()\n",
    "\n",
    "    # Get all text predictions\n",
    "    texts = batch_recognition(\n",
    "        images=textbox_images,\n",
    "        # Weird, but we need to tell the model that every\n",
    "        # images is in Thai and English\n",
    "        languages=[[\"th\", \"en\"]] * len(textbox_images),\n",
    "        model=recognizer,\n",
    "        processor=recognizer_processor,\n",
    "        batch_size=1\n",
    "    )[0]\n",
    "\n",
    "    document_info = {}\n",
    "    for text, class_prediction in zip(texts, class_predictions):\n",
    "        # Get class name\n",
    "        predicted_class = class_names[class_prediction]\n",
    "        # A little bit of cleaning\n",
    "        text = \" \".join(text.split())  # Remove extra whitespaces\n",
    "        text = text.strip()  # Remove leading and trailing whitespaces\n",
    "        # Save the data\n",
    "        document_info[predicted_class] = text\n",
    "    \n",
    "    surya_document_data.append(document_info)\n",
    "\n",
    "# Fill in missing keys\n",
    "for document_info in surya_document_data:\n",
    "    for key in class_names:\n",
    "        if key not in document_info:\n",
    "            document_info[key] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Construct the dataframe\n",
    "surya_prediction_df = pd.DataFrame(surya_document_data)\n",
    "# Convert YOLO class names to the actual column names\n",
    "convert_column_name_dict = {\n",
    "    'จำนวน (cylinders)': 'cylinders',\n",
    "    'น้ำหนักรถ (unladen_weight)': 'unladen_weight',\n",
    "    'เลขถังแก๊ส (fuel_tank_number)': 'fuel_tank_number',\n",
    "    'แรงม้า (horse_power)': 'horse_power',\n",
    "    'สี (color)': 'color',\n",
    "    'แบบ (model)': 'model',\n",
    "    'น้ำหนักบรรทุก/น้ำหนักเพลา (load_capacity)': 'load_capacity',\n",
    "    'เลขตัวรถ (chassis_number)': 'chassis_number',\n",
    "    'ยี่ห้อเครื่องยนต์ (engine_manufacturer)': 'engine_manufacturer',\n",
    "    'ยี่ห้อรถ (manufacturer)': 'manufacturer',\n",
    "    'ซีซี (cubic_capacity)': 'cubic_capacity',\n",
    "    'อยู่ที่ (chassis_location)': 'chassis_location',\n",
    "    'จังหวัด (car_province)': 'car_province',\n",
    "    'ลักษณะ (body_style)': 'body_style',\n",
    "    'รย (type)': 'type',\n",
    "    'วันจดทะเบียน (date_of_registration)': 'date_of_registration',\n",
    "    'ประเภท (vehicle_use)': 'vehicle_use',\n",
    "    'น้ำหนักรวม (gross_weight)': 'gross_weight',\n",
    "    'เลขเครื่องยนต์ (engine_number)': 'engine_number',\n",
    "    'รุ่นปี คศ (year)': 'year',\n",
    "    'ที่นั่ง (seats)': 'seats',\n",
    "    'เลขทะเบียน (registration_no)': 'registration_no',\n",
    "    'อยู่ที่ (engine_location)': 'engine_location',\n",
    "    'จำนวนเพลาและล้อ (axles_wheels_no)': 'axles_wheels_no',\n",
    "    'เชื้อเพลิง (fuel_type)': 'fuel_type'\n",
    "}\n",
    "\n",
    "# Rename columns in the dataframe\n",
    "surya_prediction_df.rename(columns=convert_column_name_dict, inplace=True)\n",
    "# Fill in missing columns\n",
    "surya_prediction_df.fillna(\"\", inplace=True)\n",
    "# Post-process the dataframe\n",
    "surya_prediction_df = postprocess_df(surya_prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(surya_prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "annotated_df = pd.read_excel('../datasets/Data/annotation_sawad_100.xlsx', dtype=str).fillna(\"\")\n",
    "# Sort document_name\n",
    "annotated_df = annotated_df.sort_values(\"document_name\")\n",
    "annotated_df.drop(columns=[\"document_id\", \"document_name\"], inplace=True)\n",
    "eval_df = evaluate(annotated_df, surya_prediction_df)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EasyOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyocr import Reader\n",
    "\n",
    "reader = Reader([\"th\", \"en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "easyocr_document_data = []\n",
    "# Detect bboxes for each document\n",
    "for result in tqdm(yolo_results):\n",
    "    # Get textboxes\n",
    "    textbox_images = get_textboxes(result)\n",
    "\n",
    "    # Get bounding boxes, class labels, and scores\n",
    "    class_names = result.names\n",
    "    class_predictions = result.boxes.cls.cpu().int().numpy()\n",
    "\n",
    "    # Get all text predictions\n",
    "    texts = [reader.recognize(np.array(textbox)) for textbox in textbox_images]\n",
    "\n",
    "    document_info = {}\n",
    "    for text, class_prediction in zip(texts, class_predictions):\n",
    "        # Get class name\n",
    "        predicted_class = class_names[class_prediction]\n",
    "        _, text, _ = text[0]\n",
    "        # A little bit of cleaning\n",
    "        text = \" \".join(text.split())  # Remove extra whitespaces\n",
    "        text = text.strip()  # Remove leading and trailing whitespaces\n",
    "        # Save the data\n",
    "        document_info[predicted_class] = text\n",
    "    \n",
    "    easyocr_document_data.append(document_info)\n",
    "\n",
    "# Fill in missing keys\n",
    "for document_info in easyocr_document_data:\n",
    "    for key in class_names:\n",
    "        if key not in document_info:\n",
    "            document_info[key] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Construct the dataframe\n",
    "easyocr_prediction_df = pd.DataFrame(easyocr_document_data)\n",
    "# Convert YOLO class names to the actual column names\n",
    "convert_column_name_dict = {\n",
    "    'จำนวน (cylinders)': 'cylinders',\n",
    "    'น้ำหนักรถ (unladen_weight)': 'unladen_weight',\n",
    "    'เลขถังแก๊ส (fuel_tank_number)': 'fuel_tank_number',\n",
    "    'แรงม้า (horse_power)': 'horse_power',\n",
    "    'สี (color)': 'color',\n",
    "    'แบบ (model)': 'model',\n",
    "    'น้ำหนักบรรทุก/น้ำหนักเพลา (load_capacity)': 'load_capacity',\n",
    "    'เลขตัวรถ (chassis_number)': 'chassis_number',\n",
    "    'ยี่ห้อเครื่องยนต์ (engine_manufacturer)': 'engine_manufacturer',\n",
    "    'ยี่ห้อรถ (manufacturer)': 'manufacturer',\n",
    "    'ซีซี (cubic_capacity)': 'cubic_capacity',\n",
    "    'อยู่ที่ (chassis_location)': 'chassis_location',\n",
    "    'จังหวัด (car_province)': 'car_province',\n",
    "    'ลักษณะ (body_style)': 'body_style',\n",
    "    'รย (type)': 'type',\n",
    "    'วันจดทะเบียน (date_of_registration)': 'date_of_registration',\n",
    "    'ประเภท (vehicle_use)': 'vehicle_use',\n",
    "    'น้ำหนักรวม (gross_weight)': 'gross_weight',\n",
    "    'เลขเครื่องยนต์ (engine_number)': 'engine_number',\n",
    "    'รุ่นปี คศ (year)': 'year',\n",
    "    'ที่นั่ง (seats)': 'seats',\n",
    "    'เลขทะเบียน (registration_no)': 'registration_no',\n",
    "    'อยู่ที่ (engine_location)': 'engine_location',\n",
    "    'จำนวนเพลาและล้อ (axles_wheels_no)': 'axles_wheels_no',\n",
    "    'เชื้อเพลิง (fuel_type)': 'fuel_type'\n",
    "}\n",
    "\n",
    "# Rename columns in the dataframe\n",
    "easyocr_prediction_df.rename(columns=convert_column_name_dict, inplace=True)\n",
    "# Fill in missing columns\n",
    "easyocr_prediction_df.fillna(\"\", inplace=True)\n",
    "# Post-process the dataframe\n",
    "easyocr_prediction_df = postprocess_df(easyocr_prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "annotated_df = pd.read_excel('../datasets/Data/annotation_sawad_100.xlsx', dtype=str).fillna(\"\")\n",
    "annotated_df = annotated_df.sort_values(\"document_name\")\n",
    "annotated_df.drop(columns=[\"document_id\", \"document_name\"], inplace=True)\n",
    "eval_df = evaluate(annotated_df, easyocr_prediction_df)\n",
    "eval_df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
