{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Detection with EasyOCR\n",
    "\n",
    "ใน Notebook นี้เราจะสาธิตวิธีการใช้ไลบรารี่ []`EasyOCR`](https://github.com/JaidedAI/EasyOCR) เพื่อตรวจจับข้อความในรูปภาพ EasyOCR เป็นไลบรารี Python ที่สามารถทำการรู้จำตัวอักษร (OCR) ได้ด้วยโค้ดเพียงไม่กี่บรรทัด\n",
    "\n",
    "EasyOCR เป็นเครื่องมือที่ใช้งานง่ายสำหรับการทำ OCR โดยที่รองรับหลายภาษารวมถึงภาษาไทย, เหมาะสำหรับผู้เริ่มต้นใช้งาน OCR, และยังสามารถแยกส่วนของโมเดลมาใช้งานได้\n",
    "\n",
    "## การติดตั้ง\n",
    "\n",
    "เราสามารถติดตั้งไลบรารี EasyOCR โดยสามารถติดตั้งผ่าน pip ได้ด้วยคำสั่ง `pip install easyocr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install EasyOCR\n",
    "%%capture\n",
    "!pip install easyocr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## การเริ่มต้นใช้งาน EasyOCR Reader\n",
    "\n",
    "ไลบรารี EasyOCR รองรับการทำงานได้หลายภาษา ในที่นี้เราจะเริ่มต้นตัว `Reader` ด้วยภาษาอังกฤษและภาษาไทย (`en`, `th`)\n",
    "\n",
    "หมายเหตุ: การระบุภาษาที่ใส่เข้าไปนี้จะถูกใช้สำหรับกระบวนการ **รู้จำ** ข้อความ (recognition) เท่านั้น แต่ไม่ได้ใช้สำหรับกระบวนการ **ตรวจจับ** ข้อความ (detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyocr import Reader\n",
    "\n",
    "# Initialize the EasyOCR reader\n",
    "reader = Reader([\"en\", \"th\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Display the Image\n",
    "\n",
    "We will load the our document image and display it using jupyter notebook built-in `display` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Load the image from a path\n",
    "image_path = \"path/to/image.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Text Detection\n",
    "\n",
    "We will use the `readtext` method from the `EasyOCR` reader to detect text in the image. This method returns a list of results, where each result contains the bounding box, the detected text, and the confidence score.\n",
    "\n",
    "```py\n",
    "[[[x1, y1], [x2, y2], [x3, y3], [x4, y4]], 'Detected Text', 0.99]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform text detection\n",
    "results = reader.readtext(image_path)\n",
    "\n",
    "# Print first result just to see the output\n",
    "bbox, text, confidence_score = results[0]\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Confidence score: {confidence_score}\")\n",
    "print(f\"Bounding box: {bbox}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw Bounding Boxes and Display the Image with Predictions\n",
    "\n",
    "We will draw the bounding boxes around the detected text and display the image with the text predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw, ImageFont\n",
    "\n",
    "FONT = ImageFont.truetype(\"../assets/THSarabun.ttf\", size=20)\n",
    "\n",
    "\n",
    "def draw_boxes(image: Image.Image, results: list[tuple[int, int, int, int], str, float]) -> None:\n",
    "    \"\"\"Draw bounding boxes and its information on the image.\"\"\"\n",
    "    # Create a drawing object\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Draw each result on the image.\n",
    "    for result in results:\n",
    "        # Unpack the result.\n",
    "        bbox, text, confidence_score = result\n",
    "\n",
    "        # bbox is a four-point coordinate of the bounding box. [[x1, y1], [x2, y2], [x3, y3], [x4, y4]]\n",
    "        # we need to convert it to PIL coordinates to draw the rectangle.\n",
    "        # which is only two points, top-left and bottom-right. [x1, y1, x2, y2]\n",
    "\n",
    "        pil_bbox = [bbox[0][0], bbox[0][1], bbox[2][0], bbox[2][1]]\n",
    "\n",
    "        # 1. Draw the bounding box\n",
    "        draw.rectangle(pil_bbox, outline=\"blue\", width=2)\n",
    "        # 2. Draw the text and confidence score e.g. 'Hello (0.72)'\n",
    "        draw_text = f\"{text} ({confidence_score:.2f})\"\n",
    "\n",
    "        # Place text at the top-left of the bounding box and shift it up by 20 pixels\n",
    "        x = pil_bbox[0]\n",
    "        y = pil_bbox[1] - 20\n",
    "        draw.text((x, y), draw_text, fill=\"red\", font=FONT)\n",
    "\n",
    "\n",
    "# Create a copy of the image to draw on\n",
    "image_with_boxes = image.copy()\n",
    "draw_boxes(image_with_boxes, results)\n",
    "\n",
    "display(image_with_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `easyocr.Reader` to perform only text detection task by using the `detect` method. This allows us to use custom text recognition model rather than easyocr's default model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyocr import Reader\n",
    "\n",
    "# Initialize the EasyOCR reader\n",
    "detector = Reader([])  # Don't need to specify any languages because we only need to detect the text regions\n",
    "\n",
    "# Perform text detection\n",
    "batch_regions, _ = detector.detect(image_path)\n",
    "\n",
    "# Because we only inference single image, we only need the first result\n",
    "regions = batch_regions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first result just to see the output\n",
    "bbox = regions[0]\n",
    "\n",
    "print(f\"Bounding box: {bbox}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can draw the bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "\n",
    "# Create a copy of the image to draw on\n",
    "image_with_boxes = image.copy()\n",
    "\n",
    "# Create a drawing object\n",
    "draw = ImageDraw.Draw(image_with_boxes)\n",
    "\n",
    "# Draw each region on the image\n",
    "for region in regions:\n",
    "    # Rearrange the region to match the PIL coordinate system\n",
    "    region = [region[0], region[2], region[1], region[3]]\n",
    "    draw.rectangle(region, outline=\"blue\", width=2)\n",
    "\n",
    "display(image_with_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Information from the recognized text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create extract question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_question = \"\"\"\n",
    "You are provided with a recognized text from the OCR system of a Thai vehicle registration book (สมุดทะเบียนรถ), which each recognized text are seperated by tab (\\t) character.\n",
    "Your task is to extract the following information from the image.\n",
    "The extracted value is typically located on the right side of the key in the document.\n",
    "Some of the text might be corrupted, missing diacritics or misread, autocorrection is appreciated.\n",
    "Extract these details:\n",
    "\n",
    "1. วันจดทะเบียน (date_of_registration)\n",
    "2. เลขทะเบียน (registration_no)\n",
    "3. จังหวัด (car_province)\n",
    "4. ประเภท (vehicle_use)\n",
    "5. รย. (type)\n",
    "6. ลักษณะ (body_style)\n",
    "7. ยี่ห้อรถ (manufacturer)\n",
    "8. แบบ (model)\n",
    "9. รุ่นปี คศ (year)\n",
    "10. สี (color)\n",
    "11. เลขตัวรถ (chassis_number)\n",
    "12. อยู่ที่ (chassis_location)\n",
    "13. ยี่ห้อเครื่องยนต์ (engine_manufacturer)\n",
    "14. เลขเครื่องยนต์ (engine_number)\n",
    "15. อยู่ที่ (engine_location)\n",
    "16. เชื้อเพลิง (fuel_type)\n",
    "17. เลขถังแก๊ส (fuel_tank_number)\n",
    "18. จำนวน (cylinders)\n",
    "19. ซีซี (cubic_capacity)\n",
    "20. แรงม้า (horse_power)\n",
    "21. จำนวนเพลาและล้อ (axles_wheels_no)\n",
    "22. น้ำหนักรถ (unladen_weight)\n",
    "23. น้ำหนักบรรทุก/น้ำหนักเพลา (load_capacity)\n",
    "24. น้ำหนักรวม (gross_weight)\n",
    "25. ที่นั่ง (seats)\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Carefully examine the image and locate each piece of information.\n",
    "If a particular field is not visible or not present in the image, use the value \"N/A\" for that field.\n",
    "Ensure all text extracted from the image is in its original language (Thai or English) as it appears in the document.\n",
    "Return the extracted information in a JSON format, using the English key names provided in parentheses.\n",
    "Only return the JSON output, without any additional explanation or text.\n",
    "\n",
    "Example of expected output in dictionary format:\n",
    "{\n",
    "  \"date_of_registration\": \"1 ม.ค. 2566\",\n",
    "  \"registration_no\": \"กข 1234\",\n",
    "  \"car_province\": \"กรุงเทพมหานคร\",\n",
    "  ...\n",
    "  \"seats\": \"4\"\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform information extraction with Llama3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack recognized text into a single string for prompting\n",
    "recognized_text = \"\\t\".join([result[1] for result in results])\n",
    "recognized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "llm = Ollama(model=\"llama3.1\", stop=[\"<|eot_id|>\"]) # Added stop token\n",
    "\n",
    "def get_model_response(user_prompt, system_prompt):\n",
    "    # NOTE: No f string and no whitespace in curly braces\n",
    "    template = \"\"\"\n",
    "        <|begin_of_text|>\n",
    "        <|start_header_id|>system<|end_header_id|>\n",
    "        {system_prompt}\n",
    "        <|eot_id|>\n",
    "        <|start_header_id|>user<|end_header_id|>\n",
    "        {user_prompt}\n",
    "        <|eot_id|>\n",
    "        <|start_header_id|>assistant<|end_header_id|>\n",
    "        \"\"\"\n",
    "\n",
    "    # Added prompt template\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"system_prompt\", \"user_prompt\"],\n",
    "        template=template\n",
    "    )\n",
    "\n",
    "    # Modified invoking the model\n",
    "    response = llm(prompt.format(system_prompt=system_prompt, user_prompt=user_prompt))\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "user_prompt = recognized_text\n",
    "system_prompt = extract_question\n",
    "answer = get_model_response(user_prompt, system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
